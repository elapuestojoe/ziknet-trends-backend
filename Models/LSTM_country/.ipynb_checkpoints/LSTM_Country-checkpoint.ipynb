{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Concatenate, Flatten, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import logcosh\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "%matplotlib inline\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Population values\n",
    "population = {\n",
    "    \"Chiapas_2016-2017.csv\": 5217908,\n",
    "    \"Colima_2016-2017.csv\": 711235,\n",
    "    \"Guerrero_2016-2017.csv\": 3533251,\n",
    "    \"Hidalgo_2016-2017.csv\": 2858359,\n",
    "    \"NuevoLeon_2016-2017.csv\": 5119504,\n",
    "    \"Oaxaca_2016-2017.csv\": 3967889,\n",
    "    \"QuintanaRoo_2016-2017.csv\": 1501562,\n",
    "    \"Tabasco_2016-2017.csv\" : 2395272,\n",
    "    \"Veracruz_2016-2017.csv\" : 8112505,\n",
    "    \"Yucatan_2016-2017.csv\" : 2097175,\n",
    "    \n",
    "    \"casanare_2016-2017.csv\" : 356438,\n",
    "    \"cordoba_2016-2017.csv\" : 1709603,\n",
    "    \"cundinamarca_2016-2017.csv\" : 2680041,\n",
    "    \"huila_2016-2017.csv\" : 1154804,\n",
    "    \"meta_2016-2017.csv\" : 961292,\n",
    "    \"santander_2016-2017.csv\" : 2061095,\n",
    "    \"santander_norte_2016-2017.csv\" : 1355723,\n",
    "    \"tolima_2016-2017.csv\" : 1408274,\n",
    "    \"valle_cauca_2016-2017.csv\" : 4613377,\n",
    "    \n",
    "    \"Alagoas_2016-2017.csv\": 3375823,\n",
    "    \"Bahia_2016-2017.csv\": 15344447,\n",
    "    \"Ceara_2016-2017.csv\": 9020460,\n",
    "    \"Goias_2016-2017.csv\": 6778772,\n",
    "    \"Maranhao_2016-2017.csv\": 7000229,\n",
    "    \"MatoGrosso_2016-2017.csv\": 3344544,\n",
    "    \"MinasGerais_2016-2017.csv\": 21119536,\n",
    "    \"Para_2016-2017.csv\": 8366628,\n",
    "    \"RioDeJaneiro_2016-2017.csv\": 16718956,\n",
    "    \"SaoPaulo_2016-2017.csv\": 45094866,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getXY(dataset, scale):\n",
    "    dataset[[\"Searches\"]] /= 100\n",
    "    dataset[[\"Cases\"]] = dataset[[\"Cases\"]].apply(lambda x: x*100000/scale, axis=1)\n",
    "    \n",
    "    dataset.drop([\"Date\"], axis=1, inplace=True)\n",
    "    values = dataset.values.astype(\"float32\")\n",
    "    \n",
    "    \n",
    "\n",
    "    n_weeks = 4\n",
    "    n_features = 2\n",
    "\n",
    "    reframed = series_to_supervised(values, n_weeks, 1)\n",
    "    values = reframed.values\n",
    "    print(\"Reframed Shape: \", reframed.shape)\n",
    "    totalFeatures = reframed.shape[1]\n",
    "    n_obs = n_weeks * n_features\n",
    "\n",
    "    x, y = values[:, :-2], values[:, -1] # Pick last week's cases as y and drop last week's \n",
    "\n",
    "    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveModel(model, modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(jsonName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #seralize weights to HDF5\n",
    "    model.save_weights(h5Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(4,2))\n",
    "    b1_out = LSTM(512, return_sequences=True)(input_layer)\n",
    "#     b1_out = LSTM(512, activation=\"tanh\", return_sequences=True)(b1_out)\n",
    "    b1_out = LSTM(256, activation=\"tanh\", return_sequences=False)(b1_out)\n",
    "    \n",
    "    model1 = Model(input_layer, b1_out)\n",
    "    \n",
    "    b2_out = Dense(1024, activation=\"tanh\")(input_layer)\n",
    "#     b2_out = Dense(256, activation=\"tanh\")(b2_out)\n",
    "    b2_out = Flatten()(b2_out)\n",
    "    \n",
    "    model2 = Model(input_layer, b2_out)\n",
    "    \n",
    "    concatenated = concatenate([b1_out, b2_out])\n",
    "    out = Dense(128, activation=\"tanh\")(concatenated)\n",
    "    out = Dense(1, activation='linear', name='output_layer')(concatenated)\n",
    "    \n",
    "    model = Model([input_layer], out)\n",
    "    model.compile(loss=[\"mae\"], optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiapas_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Colima_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Guerrero_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Hidalgo_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "NuevoLeon_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Oaxaca_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "QuintanaRoo_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Tabasco_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Veracruz_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n",
      "Yucatan_2016-2017.csv\n",
      "Reframed Shape:  (100, 10)\n"
     ]
    }
   ],
   "source": [
    "countries = [\"Mexico\", \"Brazil\", \"Colombia\"]\n",
    "\n",
    "for country in countries:\n",
    "    otherCountries = [x for x in countries if x != country]\n",
    "    \n",
    "    model = createModel(country)\n",
    "    #Train\n",
    "    folder = \"../../data/{}/processed_data\".format(country)\n",
    "    files = os.listdir(folder)\n",
    "    with tf.Session() as sess:\n",
    "        for file in files:\n",
    "            dataset = pd.read_csv(\"{}/{}\".format(folder, file))\n",
    "            state = file\n",
    "            print(file)\n",
    "            x, y = getXY(dataset, population[file])\n",
    "            model.fit(x, y,\n",
    "                    epochs = 200,\n",
    "                    batch_size=x.shape[0],\n",
    "                    verbose=0, \n",
    "                    shuffle=True\n",
    "                     )\n",
    "    #Test\n",
    "    \n",
    "    for otherCountry in otherCountries:\n",
    "        otherFolder = \"../../data/{}/processed_data\".format(otherCountry)\n",
    "        testFiles = os.listdir(otherFolder)\n",
    "        with tf.Session() as sess:\n",
    "            for file in testFiles:\n",
    "\n",
    "                    test_dataset = pd.read_csv(\"{}/{}\".format(folder, file))\n",
    "                    state = file\n",
    "                    if(not os.path.isdir(\"{}/{}\".format(country, otherCountry))):\n",
    "                        os.mkdir(\"{}/{}\".format(country, otherCountry))\n",
    "\n",
    "                    test_x, test_y = getXY(train, population[file])\n",
    "                    predictions = model.predict(test_x)\n",
    "                    #Transform to 1-D\n",
    "                    test_y = test_y.reshape((len(test_y), 1))\n",
    "\n",
    "                    #Rescale\n",
    "                    inv_yPred = np.apply_along_axis(lambda x: x * population[file] / 100000, 1, predictions)\n",
    "                    inv_y = np.apply_along_axis(lambda x: x * population[file] / 100000, 1, test_y)\n",
    "\n",
    "                    test_dataset = test_dataset[4:]\n",
    "                    test_dataset[\"predictions\"] = inv_yPred\n",
    "                    test_dataset[\"error\"] = test_dataset[\"Cases\"] - test_dataset[\"predictions\"]\n",
    "\n",
    "                    test.to_csv(\"{}/{}/{}\".format(country, otherCountry, file))\n",
    "\n",
    "                    test[[\"Cases\", \"predictions\"]].plot(figsize=(10,10))\n",
    "                    plt.title(file)\n",
    "                    plt.xlabel(\"Week\")\n",
    "                    plt.ylabel(\"Cases\")\n",
    "                    plt.legend()\n",
    "                    plt.grid()\n",
    "                    fig = plt.gcf()\n",
    "                    fig.savefig(\"{}/{}/Obs-Pred.png\".format(country, file))\n",
    "                    plt.close(\"all\")\n",
    "    \n",
    "    saveModel(model, \"{}/{}/Model\".format(country, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
