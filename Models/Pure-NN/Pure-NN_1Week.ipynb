{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Concatenate, Flatten, Input, GRU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.constraints import non_neg\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Population values\n",
    "population = {\n",
    "    \"Chiapas_2016-2017.csv\": 5217908,\n",
    "    \"Colima_2016-2017.csv\": 711235,\n",
    "    \"Guerrero_2016-2017.csv\": 3533251,\n",
    "    \"Hidalgo_2016-2017.csv\": 2858359,\n",
    "    \"NuevoLeon_2016-2017.csv\": 5119504,\n",
    "    \"Oaxaca_2016-2017.csv\": 3967889,\n",
    "    \"QuintanaRoo_2016-2017.csv\": 1501562,\n",
    "    \"Tabasco_2016-2017.csv\" : 2395272,\n",
    "    \"Veracruz_2016-2017.csv\" : 8112505,\n",
    "    \"Yucatan_2016-2017.csv\" : 2097175,\n",
    "    \n",
    "    \"casanare_2016-2017.csv\" : 356438,\n",
    "    \"cordoba_2016-2017.csv\" : 1709603,\n",
    "    \"cundinamarca_2016-2017.csv\" : 2680041,\n",
    "    \"huila_2016-2017.csv\" : 1154804,\n",
    "    \"meta_2016-2017.csv\" : 961292,\n",
    "    \"santander_2016-2017.csv\" : 2061095,\n",
    "    \"santander_norte_2016-2017.csv\" : 1355723,\n",
    "    \"tolima_2016-2017.csv\" : 1408274,\n",
    "    \"valle_cauca_2016-2017.csv\" : 4613377,\n",
    "    \n",
    "    \"Alagoas_2016-2017.csv\": 3375823,\n",
    "    \"Bahia_2016-2017.csv\": 15344447,\n",
    "    \"Ceara_2016-2017.csv\": 9020460,\n",
    "    \"Goias_2016-2017.csv\": 6778772,\n",
    "    \"Maranhao_2016-2017.csv\": 7000229,\n",
    "    \"MatoGrosso_2016-2017.csv\": 3344544,\n",
    "    \"MinasGerais_2016-2017.csv\": 21119536,\n",
    "    \"Para_2016-2017.csv\": 8366628,\n",
    "    \"RioDeJaneiro_2016-2017.csv\": 16718956,\n",
    "    \"SaoPaulo_2016-2017.csv\": 45094866,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveModel(model, modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(jsonName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #seralize weights to HDF5\n",
    "    model.save_weights(h5Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(4,2))\n",
    "    b1_out = Dense(64)(input_layer)\n",
    "    b1_out = Flatten()(b1_out)\n",
    "    \n",
    "    b2_out = Dense(32, activation=\"relu\", kernel_regularizer=\"l2\")(input_layer)\n",
    "    b2_out = Flatten()(b2_out)\n",
    "    \n",
    "    concatenated = concatenate([b1_out, b2_out])\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(concatenated)\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(out)\n",
    "    out = Dense(1, activation=\"linear\", kernel_constraint=non_neg(), name='output_layer')(out)\n",
    "    \n",
    "    model = Model([input_layer], out)\n",
    "    model.compile(loss=[\"mae\"], optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getXY(dataset, scale):\n",
    "    dataset[[\"Searches\"]] /= 100\n",
    "    dataset[[\"Cases\"]] = dataset[[\"Cases\"]].apply(lambda x: x*100000/scale, axis=1)\n",
    "\n",
    "    values = dataset.values.astype(\"float32\")\n",
    "    \n",
    "    n_weeks = 4\n",
    "    n_features = 2\n",
    "\n",
    "    reframed = series_to_supervised(values, n_weeks, 1)\n",
    "    values = reframed.values\n",
    "    print(reframed.columns)\n",
    "    \n",
    "    print(\"Reframed Shape: \", reframed.shape)\n",
    "    totalFeatures = reframed.shape[1]\n",
    "    n_obs = n_weeks * n_features\n",
    "\n",
    "    x,y = values[:, :8], values[:, -1] # Pick 4 previous weeks and predict 4 week ahead \n",
    "\n",
    "    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatFilename(filename):\n",
    "    return filename.replace(\".csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSortedFiles(folder, files):\n",
    "    filesArray = []\n",
    "    for file in files:\n",
    "        dataset = pd.read_csv(\"{}/{}\".format(folder,file))\n",
    "        filesArray.append([file, dataset[\"Cases\"].sum()])\n",
    "    s = sorted(filesArray, key=lambda x: x[1])\n",
    "    result = []\n",
    "    for file, cases in s:\n",
    "        result.append(file)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictExport(country, otherCountries, train=True):\n",
    "    \n",
    "    trainingType = \"test\"\n",
    "    if(train):\n",
    "        index=0\n",
    "        trainingType = \"train\"\n",
    "        \n",
    "    if(not os.path.isdir(\"{}/{}\".format(country, trainingType))):\n",
    "            os.mkdir(\"{}/{}\".format(country, trainingType))\n",
    "            \n",
    "    comparisonCsv = pd.DataFrame(columns=[\"File\", \"NaiveRMSE\", \"NNRMSE\"])\n",
    "    for otherCountry in otherCountries:\n",
    "        np.random.seed(2018)\n",
    "        folder = \"../../data/{}/processed_data\".format(otherCountry)\n",
    "        files = os.listdir(folder)\n",
    "        \n",
    "        if(not os.path.isdir(\"{}/{}/{}\".format(country, trainingType, otherCountry))):\n",
    "            os.mkdir(\"{}/{}/{}\".format(country, trainingType, otherCountry))\n",
    "        \n",
    "            \n",
    "        for file in getSortedFiles(folder, files):\n",
    "            #Validate folders\n",
    "            if(not os.path.isdir(\"{}/{}/{}/{}\".format(country, trainingType, otherCountry, file))):\n",
    "                os.mkdir(\"{}/{}/{}/{}\".format(country, trainingType, otherCountry, file))\n",
    "            \n",
    "            dataset = pd.read_csv(\"{}/{}\".format(folder, file), index_col=0)\n",
    "            x, y = getXY(dataset, population[file])\n",
    "\n",
    "            predictions = model.predict(x)\n",
    "\n",
    "            #Transform to 1-D\n",
    "            y = y.reshape((len(y), 1))\n",
    "\n",
    "            #Rescale\n",
    "            inv_yPred = np.apply_along_axis(lambda x: x * population[file] / 100000, 1, predictions)\n",
    "\n",
    "            dataset[\"Cases\"] *= (population[file] / 100000)\n",
    "            naive = dataset[\"Cases\"].values[3:-1]\n",
    "            dataset = dataset[4:]\n",
    "\n",
    "            dataset[\"NN-Prediction\"] = inv_yPred\n",
    "\n",
    "            dataset[\"NNError\"] = dataset[\"NN-Prediction\"] - dataset[\"Cases\"]\n",
    "\n",
    "            #Naive\n",
    "            dataset[\"Naive-Prediction\"] = naive\n",
    "            dataset[\"NaiveError\"] = dataset[\"Naive-Prediction\"] - dataset[\"Cases\"]\n",
    "            \n",
    "            \n",
    "            naiveErrorSquared = dataset[\"NaiveError\"] ** 2\n",
    "            naiveMSE = naiveErrorSquared.mean()\n",
    "            naiveRMSE = naiveMSE ** (0.5)\n",
    "            \n",
    "            NNErrorSquared = dataset[\"NNError\"] ** 2\n",
    "            NNMSE = NNErrorSquared.mean()\n",
    "            NNRMSE = NNMSE ** (0.5)\n",
    "\n",
    "            comparisonCsv = comparisonCsv.append({\n",
    "                \"File\": file,\n",
    "                \"NaiveRMSE\": naiveRMSE,\n",
    "                \"NNRMSE\" : NNRMSE,\n",
    "                                 }, ignore_index=True)\n",
    "\n",
    "            #Plots\n",
    "            dataset.rename(index=str, columns={\"Cases\": \"Observed\"}, inplace=True)\n",
    "\n",
    "            dataset.to_csv(\"{}/{}/{}/{}/{}\".format(country, trainingType, otherCountry, file, file))\n",
    "\n",
    "            dataset[[\"Observed\", \"NN-Prediction\", \"Naive-Prediction\"]].plot(figsize=(10,10))\n",
    "            plt.title(\"NN Model\\n{}\".format(file))\n",
    "            ax = plt.gca()\n",
    "            ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"ZIKV Cases\")\n",
    "            plt.legend()\n",
    "            plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "            fig = plt.gcf()\n",
    "            fig.savefig(\"{}/{}/{}/{}/{}.png\".format(country, trainingType, otherCountry, file, file))\n",
    "            plt.close(\"all\")\n",
    "            \n",
    "            #Plot error\n",
    "            plt.clf()\n",
    "            dataset[[\"NNError\", \"NaiveError\"]].plot(figsize=(10,10))\n",
    "            plt.title(\"NN Error\\n{}\".format(file))\n",
    "            ax = plt.gca()\n",
    "            ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Error\")\n",
    "            plt.legend()\n",
    "            plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "            fig = plt.gcf()\n",
    "            fig.savefig(\"{}/{}/{}/{}/Error-{}.png\".format(country, trainingType, otherCountry, file, file))\n",
    "            plt.close(\"all\")         \n",
    "\n",
    "    comparisonCsv[\"NN-Naive-Ratio\"] = comparisonCsv[\"NNRMSE\"] / comparisonCsv[\"NaiveRMSE\"]\n",
    "\n",
    "    comparisonCsv[\"Average-Ratio\"] = comparisonCsv[\"NNRMSE\"].sum() / comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv[\"TotalNN-RMSE\"] = comparisonCsv[\"NNRMSE\"].sum()\n",
    "    comparisonCsv[\"TotalNaive-RMSE\"] = comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv.to_csv(\"{}/{}/RMSE.csv\".format(country, trainingType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = input(\"Country: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Mexico\", \"Colombia\", \"Brazil\"]\n",
    "otherCountries = [x for x in countries if x != country]\n",
    "\n",
    "model = createModel(country)\n",
    "#Train\n",
    "folder = \"../../data/{}/processed_data\".format(country)\n",
    "files = os.listdir(folder)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for file in getSortedFiles(folder, files):\n",
    "        dataset = pd.read_csv(\"{}/{}\".format(folder, file), index_col=0)\n",
    "        state = file\n",
    "        print(file)\n",
    "        x, y = getXY(dataset, population[file])\n",
    "        model.fit(x, y,\n",
    "                epochs =60,\n",
    "                batch_size=x.shape[0],\n",
    "                verbose=0, \n",
    "                shuffle=False\n",
    "                 )\n",
    "    PredictExport(country, [country], train=True)   \n",
    "    \n",
    "    PredictExport(country, otherCountries, train=False)\n",
    "    saveModel(model, \"{}/Model\".format(country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
