{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Concatenate, Flatten, Input, GRU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.constraints import non_neg\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Population values\n",
    "population = {\n",
    "    \"Chiapas_2016-2017.csv\": 5217908,\n",
    "    \"Colima_2016-2017.csv\": 711235,\n",
    "    \"Guerrero_2016-2017.csv\": 3533251,\n",
    "    \"Hidalgo_2016-2017.csv\": 2858359,\n",
    "    \"NuevoLeon_2016-2017.csv\": 5119504,\n",
    "    \"Oaxaca_2016-2017.csv\": 3967889,\n",
    "    \"QuintanaRoo_2016-2017.csv\": 1501562,\n",
    "    \"Tabasco_2016-2017.csv\" : 2395272,\n",
    "    \"Veracruz_2016-2017.csv\" : 8112505,\n",
    "    \"Yucatan_2016-2017.csv\" : 2097175,\n",
    "    \n",
    "    \"casanare_2016-2017.csv\" : 356438,\n",
    "    \"cordoba_2016-2017.csv\" : 1709603,\n",
    "    \"cundinamarca_2016-2017.csv\" : 2680041,\n",
    "    \"huila_2016-2017.csv\" : 1154804,\n",
    "    \"meta_2016-2017.csv\" : 961292,\n",
    "    \"santander_2016-2017.csv\" : 2061095,\n",
    "    \"santander_norte_2016-2017.csv\" : 1355723,\n",
    "    \"tolima_2016-2017.csv\" : 1408274,\n",
    "    \"valle_cauca_2016-2017.csv\" : 4613377,\n",
    "    \n",
    "    \"Alagoas_2016-2017.csv\": 3375823,\n",
    "    \"Bahia_2016-2017.csv\": 15344447,\n",
    "    \"Ceara_2016-2017.csv\": 9020460,\n",
    "    \"Goias_2016-2017.csv\": 6778772,\n",
    "    \"Maranhao_2016-2017.csv\": 7000229,\n",
    "    \"MatoGrosso_2016-2017.csv\": 3344544,\n",
    "    \"MinasGerais_2016-2017.csv\": 21119536,\n",
    "    \"Para_2016-2017.csv\": 8366628,\n",
    "    \"RioDeJaneiro_2016-2017.csv\": 16718956,\n",
    "    \"SaoPaulo_2016-2017.csv\": 45094866,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveModel(model, modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(jsonName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #seralize weights to HDF5\n",
    "    model.save_weights(h5Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(4,1))\n",
    "    b1_out = LSTM(64, return_sequences=False)(input_layer)\n",
    "    \n",
    "    b2_out = Dense(32, activation=\"relu\", kernel_regularizer=\"l2\")(input_layer)\n",
    "    b2_out = Flatten()(b2_out)\n",
    "    \n",
    "    concatenated = concatenate([b1_out, b2_out])\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(concatenated)\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(out)\n",
    "    out = Dense(1, activation=\"linear\", kernel_constraint=non_neg(), name='output_layer')(out)\n",
    "    \n",
    "    model = Model([input_layer], out)\n",
    "    model.compile(loss=[\"mae\"], optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getXY(dataset, scale):\n",
    "#     dataset[[\"Searches\"]] /= 100\n",
    "    dataset[[\"Cases\"]] = dataset[[\"Cases\"]].apply(lambda x: x*100000/scale, axis=1)\n",
    "\n",
    "    values = dataset[[\"Cases\"]].values.astype(\"float32\")\n",
    "    \n",
    "    n_weeks = 4\n",
    "    n_features = 1\n",
    "\n",
    "    reframed = series_to_supervised(values, n_weeks, 1)\n",
    "    values = reframed.values\n",
    "    print(reframed.columns)\n",
    "    \n",
    "    print(\"Reframed Shape: \", reframed.shape)\n",
    "    totalFeatures = reframed.shape[1]\n",
    "    n_obs = n_weeks * n_features\n",
    "\n",
    "    x,y = values[:, :4], values[:, -1] # Pick 4 previous weeks and predict 4 week ahead \n",
    "\n",
    "    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatFilename(filename):\n",
    "    return filename.replace(\".csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSortedFiles(folder, files):\n",
    "    filesArray = []\n",
    "    for file in files:\n",
    "        dataset = pd.read_csv(\"{}/{}\".format(folder,file))\n",
    "        filesArray.append([file, dataset[\"Cases\"].sum()])\n",
    "    s = sorted(filesArray, key=lambda x: x[1])\n",
    "    result = []\n",
    "    for file, cases in s:\n",
    "        result.append(file)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictExport(country, otherCountries, train=True):\n",
    "    \n",
    "    trainingType = \"test\"\n",
    "    if(train):\n",
    "        index=0\n",
    "        trainingType = \"train\"\n",
    "        \n",
    "    if(not os.path.isdir(\"{}/{}\".format(country, trainingType))):\n",
    "            os.mkdir(\"{}/{}\".format(country, trainingType))\n",
    "            \n",
    "    comparisonCsv = pd.DataFrame(columns=[\"File\", \"NaiveRMSE\", \"LSTMRMSE\"])\n",
    "    for otherCountry in otherCountries:\n",
    "        np.random.seed(2018)\n",
    "        folder = \"../../data/{}/processed_data\".format(otherCountry)\n",
    "        files = os.listdir(folder)\n",
    "        \n",
    "        if(not os.path.isdir(\"{}/{}/{}\".format(country, trainingType, otherCountry))):\n",
    "            os.mkdir(\"{}/{}/{}\".format(country, trainingType, otherCountry))\n",
    "        \n",
    "            \n",
    "        for file in getSortedFiles(folder, files):\n",
    "            #Validate folders\n",
    "            if(not os.path.isdir(\"{}/{}/{}/{}\".format(country, trainingType, otherCountry, file))):\n",
    "                os.mkdir(\"{}/{}/{}/{}\".format(country, trainingType, otherCountry, file))\n",
    "            \n",
    "            dataset = pd.read_csv(\"{}/{}\".format(folder, file), index_col=0)\n",
    "            x, y = getXY(dataset, population[file])\n",
    "\n",
    "            predictions = model.predict(x)\n",
    "\n",
    "            #Transform to 1-D\n",
    "            y = y.reshape((len(y), 1))\n",
    "\n",
    "            #Rescale\n",
    "            inv_yPred = np.apply_along_axis(lambda x: x * population[file] / 100000, 1, predictions)\n",
    "\n",
    "            dataset[\"Cases\"] *= (population[file] / 100000)\n",
    "            naive = dataset[\"Cases\"].values[3:-1]\n",
    "            dataset = dataset[4:]\n",
    "\n",
    "            dataset[\"LSTM-Prediction\"] = inv_yPred\n",
    "\n",
    "            dataset[\"LSTMError\"] = dataset[\"LSTM-Prediction\"] - dataset[\"Cases\"]\n",
    "\n",
    "            #Naive\n",
    "            dataset[\"Naive-Prediction\"] = naive\n",
    "            dataset[\"NaiveError\"] = dataset[\"Naive-Prediction\"] - dataset[\"Cases\"]\n",
    "            \n",
    "            \n",
    "            naiveErrorSquared = dataset[\"NaiveError\"] ** 2\n",
    "            naiveMSE = naiveErrorSquared.mean()\n",
    "            naiveRMSE = naiveMSE ** (0.5)\n",
    "            \n",
    "            LSTMErrorSquared = dataset[\"LSTMError\"] ** 2\n",
    "            LSTMMSE = LSTMErrorSquared.mean()\n",
    "            LSTMRMSE = LSTMMSE ** (0.5)\n",
    "\n",
    "            comparisonCsv = comparisonCsv.append({\n",
    "                \"File\": file,\n",
    "                \"NaiveRMSE\": naiveRMSE,\n",
    "                \"LSTMRMSE\" : LSTMRMSE,\n",
    "                                 }, ignore_index=True)\n",
    "\n",
    "            #Plots\n",
    "            dataset.rename(index=str, columns={\"Cases\": \"Observed\"}, inplace=True)\n",
    "\n",
    "            dataset.to_csv(\"{}/{}/{}/{}/{}\".format(country, trainingType, otherCountry, file, file))\n",
    "\n",
    "            dataset[[\"Observed\", \"LSTM-Prediction\", \"Naive-Prediction\"]].plot(figsize=(10,10))\n",
    "            plt.title(\"LSTM Model\\n{}\".format(file))\n",
    "            ax = plt.gca()\n",
    "            ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"ZIKV Cases\")\n",
    "            plt.legend()\n",
    "            plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "            fig = plt.gcf()\n",
    "            fig.savefig(\"{}/{}/{}/{}/{}.png\".format(country, trainingType, otherCountry, file, file))\n",
    "            plt.close(\"all\")\n",
    "            \n",
    "            #Plot error\n",
    "            plt.clf()\n",
    "            dataset[[\"LSTMError\", \"NaiveError\"]].plot(figsize=(10,10))\n",
    "            plt.title(\"LSTM Error\\n{}\".format(file))\n",
    "            ax = plt.gca()\n",
    "            ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Error\")\n",
    "            plt.legend()\n",
    "            plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "            fig = plt.gcf()\n",
    "            fig.savefig(\"{}/{}/{}/{}/Error-{}.png\".format(country, trainingType, otherCountry, file, file))\n",
    "            plt.close(\"all\")         \n",
    "\n",
    "    comparisonCsv[\"LSTM-Naive-Ratio\"] = comparisonCsv[\"LSTMRMSE\"] / comparisonCsv[\"NaiveRMSE\"]\n",
    "\n",
    "    comparisonCsv[\"Average-Ratio\"] = comparisonCsv[\"LSTMRMSE\"].sum() / comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv[\"TotalLSTM-RMSE\"] = comparisonCsv[\"LSTMRMSE\"].sum()\n",
    "    comparisonCsv[\"TotalNaive-RMSE\"] = comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv.to_csv(\"{}/{}/RMSE.csv\".format(country, trainingType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Brazil\n"
     ]
    }
   ],
   "source": [
    "country = input(\"Country: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SaoPaulo_2016-2017.csv\n",
      "Index(['var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)', 'var1(t)'], dtype='object')\n",
      "Reframed Shape:  (100, 5)\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(400, 1), b.shape=(1, 32), m=400, n=32, k=1\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/dense_1/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: metrics/mean_absolute_error/Mean_1/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1951_metrics/mean_absolute_error/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5057386a9c34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                  )\n\u001b[0;32m     22\u001b[0m     \u001b[0mPredictExport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(400, 1), b.shape=(1, 32), m=400, n=32, k=1\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/dense_1/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: metrics/mean_absolute_error/Mean_1/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1951_metrics/mean_absolute_error/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "countries = [\"Mexico\", \"Colombia\", \"Brazil\"]\n",
    "otherCountries = [x for x in countries if x != country]\n",
    "\n",
    "model = createModel(country)\n",
    "#Train\n",
    "folder = \"../../data/{}/processed_data\".format(country)\n",
    "files = os.listdir(folder)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for file in getSortedFiles(folder, files):\n",
    "        dataset = pd.read_csv(\"{}/{}\".format(folder, file), index_col=0)\n",
    "        state = file\n",
    "        print(file)\n",
    "        x, y = getXY(dataset, population[file])\n",
    "        model.fit(x, y,\n",
    "                epochs =30,\n",
    "                batch_size=x.shape[0],\n",
    "                verbose=0, \n",
    "                shuffle=False\n",
    "                 )\n",
    "    PredictExport(country, [country], train=True)   \n",
    "    \n",
    "    PredictExport(country, otherCountries, train=False)\n",
    "    saveModel(model, \"{}/Model\".format(country))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
