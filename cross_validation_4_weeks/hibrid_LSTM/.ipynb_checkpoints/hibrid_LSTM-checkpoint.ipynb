{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Concatenate, Flatten, Input, GRU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.constraints import non_neg\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "plotTitle = \"Pure LSTM Model\"\n",
    "modelRMSE = \"H-LSTM-RMSE\"\n",
    "modelPrediction = \"H-LSTM-Prediction\"\n",
    "modelError = \"H-LSTM-Error\"\n",
    "modelName = \"H-LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Population values\n",
    "population = {\n",
    "    \n",
    "#     Colombia\n",
    "    \"huila_2016-2017.csv\" : 1154804,\n",
    "    \"santander_2016-2017.csv\" : 2061095,\n",
    "    \"santander_norte_2016-2017.csv\" : 1355723,\n",
    "    \"tolima_2016-2017.csv\" : 1408274,\n",
    "    \"valle_cauca_2016-2017.csv\" : 4613377,\n",
    "    \n",
    "#     Brazil\n",
    "    \"Bahia_2016-2017.csv\": 15344447,\n",
    "    \"MatoGrosso_2016-2017.csv\": 3344544,\n",
    "    \"MinasGerais_2016-2017.csv\": 21119536,\n",
    "    \"RioDeJaneiro_2016-2017.csv\": 16718956,\n",
    "    \"SaoPaulo_2016-2017.csv\": 45094866,\n",
    "}\n",
    "\n",
    "countries = {\n",
    "#     Colombia\n",
    "    \"huila_2016-2017.csv\" : \"Colombia\",\n",
    "    \"santander_2016-2017.csv\" : \"Colombia\",\n",
    "    \"santander_norte_2016-2017.csv\" : \"Colombia\",\n",
    "    \"tolima_2016-2017.csv\" : \"Colombia\",\n",
    "    \"valle_cauca_2016-2017.csv\" : \"Colombia\",\n",
    "    \n",
    "#     Brazil\n",
    "    \"Bahia_2016-2017.csv\": \"Brazil\",\n",
    "    \"MatoGrosso_2016-2017.csv\": \"Brazil\",\n",
    "    \"MinasGerais_2016-2017.csv\": \"Brazil\",\n",
    "    \"RioDeJaneiro_2016-2017.csv\": \"Brazil\",\n",
    "    \"SaoPaulo_2016-2017.csv\": \"Brazil\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveModel(model, modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(jsonName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #seralize weights to HDF5\n",
    "    model.save_weights(h5Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(modelName):\n",
    "    jsonName = \"{}.json\".format(modelName)\n",
    "    h5Name = \"{}.h5\".format(modelName)\n",
    "    \n",
    "    \n",
    "    input_layer = Input(shape=(4,2))\n",
    "    b1_out = LSTM(64, return_sequences=False)(input_layer)\n",
    "    \n",
    "    b2_out = Dense(32, activation=\"relu\", kernel_regularizer=\"l2\")(input_layer)\n",
    "    b2_out = Flatten()(b2_out)\n",
    "    \n",
    "    concatenated = concatenate([b1_out, b2_out])\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(concatenated)\n",
    "    out = Dense(4, activation=\"relu\", kernel_regularizer=\"l2\")(out)\n",
    "    out = Dense(1, activation=\"linear\", kernel_constraint=non_neg(), name='output_layer')(out)\n",
    "    \n",
    "    model = Model([input_layer], out)\n",
    "    model.compile(loss=[\"mae\"], optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getXY(dataset, scale):\n",
    "    dataset[[\"Searches\"]] /= 100\n",
    "    dataset[[\"Cases\"]] = dataset[[\"Cases\"]].apply(lambda x: x*100000/scale, axis=1)\n",
    "\n",
    "    values = dataset.values.astype(\"float32\")\n",
    "    \n",
    "    n_weeks = 4\n",
    "    n_features = 2\n",
    "\n",
    "    reframed = series_to_supervised(values, n_weeks, 4)\n",
    "    values = reframed.values\n",
    "    print(reframed.columns)\n",
    "    \n",
    "    print(\"Reframed Shape: \", reframed.shape)\n",
    "    totalFeatures = reframed.shape[1]\n",
    "    n_obs = n_weeks * n_features\n",
    "\n",
    "    x,y = values[:, :8], values[:, -1] # Pick 4 previous weeks and predict 4 week ahead \n",
    "\n",
    "    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatFilename(filename):\n",
    "    return filename.replace(\".csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSortedFiles(files):\n",
    "    filesArray = []\n",
    "    for file in files:\n",
    "        country = countries[file]\n",
    "        \n",
    "        dataset = pd.read_csv(\"../../data/{}/processed_data/{}\".format(country,file))\n",
    "        filesArray.append([file, dataset[\"Cases\"].sum()])\n",
    "    s = sorted(filesArray, key=lambda x: x[1])\n",
    "    result = []\n",
    "    for file, cases in s:\n",
    "        result.append(file)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['var1(t-4)', 'var2(t-4)', 'var1(t-3)', 'var2(t-3)', 'var1(t-2)',\n",
      "       'var2(t-2)', 'var1(t-1)', 'var2(t-1)', 'var1(t)', 'var2(t)',\n",
      "       'var1(t+1)', 'var2(t+1)', 'var1(t+2)', 'var2(t+2)', 'var1(t+3)',\n",
      "       'var2(t+3)'],\n",
      "      dtype='object')\n",
      "Reframed Shape:  (97, 16)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2441\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1165895456c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mstate_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cases\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstate_population\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mnaive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cases\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mstate_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mstate_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodelPrediction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv_yPred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Aplicaciones\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7"
     ]
    }
   ],
   "source": [
    "\n",
    "states = list(population.keys())\n",
    "experiments= []\n",
    "for i in range(len(states)):\n",
    "    test_state = states[i]\n",
    "    train_states = states[:i]+states[i+1:]\n",
    "    experiments.append({\"test_state\": test_state, \"train_states\": train_states})\n",
    "testCsv = pd.DataFrame(columns=[\"File\", \"NaiveRMSE\", modelRMSE])\n",
    "for experiment in experiments:\n",
    "    foldername = experiment[\"test_state\"]\n",
    "\n",
    "    train_states = experiment[\"train_states\"]\n",
    "    train_states_sorted = getSortedFiles(train_states)\n",
    "    model = createModel(foldername)\n",
    "    if(not os.path.isdir(foldername)):\n",
    "        os.mkdir(foldername)\n",
    "\n",
    "    if(not os.path.isdir(\"{}/train\".format(foldername))):\n",
    "        os.mkdir(\"{}/train\".format(foldername))\n",
    "\n",
    "    if(not os.path.isdir(\"{}/test\".format(foldername))):\n",
    "        os.mkdir(\"{}/test\".format(foldername))\n",
    "\n",
    "    comparisonCsv = pd.DataFrame(columns=[\"File\", \"NaiveRMSE\", modelRMSE])\n",
    "    for train_state in train_states_sorted:\n",
    "        state_population = population[train_state]\n",
    "        state_country = countries[train_state]\n",
    "        data_folder = \"../../data/{}/processed_data/{}\".format(state_country, train_state)\n",
    "\n",
    "        state_dataset = pd.read_csv(data_folder, index_col=0)\n",
    "\n",
    "        x, y = getXY(state_dataset, state_population)\n",
    "\n",
    "        history = model.fit(x,y,\n",
    "                 epochs = 60,\n",
    "                 batch_size = x.shape[0],\n",
    "                 verbose = 0,\n",
    "                 shuffle=False)\n",
    "\n",
    "#         plt.clf()\n",
    "#         plt.title(\"Train - {}\".format(train_state))\n",
    "#         ax = plt.gca()\n",
    "#         ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "#         plt.plot(history.history[\"root_mean_squared_error\"])\n",
    "#         plt.ylabel(\"rmse\")\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.legend(['train'], loc=\"upper left\")\n",
    "#         fig = plt.gcf()\n",
    "#         fig.savefig(\"{}/train/Loss-{}.png\".format(foldername, train_state))\n",
    "#         plt.close(\"all\")\n",
    "\n",
    "        predictions = model.predict(x)\n",
    "        #Transform to 1-D\n",
    "        y = y.reshape((len(y), 1))\n",
    "\n",
    "        #Rescale\n",
    "        inv_yPred = np.apply_along_axis(lambda x: x * state_population / 100000, 1, predictions)\n",
    "        state_dataset[\"Cases\"] *= (state_population / 100000)\n",
    "        naive = state_dataset[\"Cases\"].values[3:-4]\n",
    "        state_dataset = state_dataset[7:\n",
    "\n",
    "        state_dataset[modelPrediction] = inv_yPred\n",
    "        state_dataset[modelError] = state_dataset[modelPrediction] - state_dataset[\"Cases\"]\n",
    "\n",
    "        #Naive\n",
    "        state_dataset[\"Naive-Prediction\"] = naive\n",
    "        state_dataset[\"NaiveError\"] = state_dataset[\"Naive-Prediction\"] - state_dataset[\"Cases\"]\n",
    "\n",
    "        naiveErrorSquared = state_dataset[\"NaiveError\"] ** 2\n",
    "        naiveMSE = naiveErrorSquared.mean()\n",
    "        naiveRMSE = naiveMSE ** (0.5)\n",
    "        NNErrorSquared = state_dataset[modelError] ** 2\n",
    "        NNMSE = NNErrorSquared.mean()\n",
    "        NNRMSE = NNMSE ** (0.5)\n",
    "\n",
    "        comparisonCsv = comparisonCsv.append({\n",
    "            \"File\": train_state,\n",
    "            \"NaiveRMSE\": naiveRMSE,\n",
    "            modelRMSE : NNRMSE,\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        state_dataset.rename(index=str, columns={\"Cases\": \"Observed\"}, inplace=True)\n",
    "\n",
    "        state_dataset.to_csv(\"{}/train/{}\".format(foldername, train_state))\n",
    "\n",
    "        state_dataset[[\"Observed\", modelPrediction, \"Naive-Prediction\"]].plot(figsize=(10,10))\n",
    "        plt.title(\"{}\\n{}\".format(plotTitle, train_state))\n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"ZIKV Cases\")\n",
    "        plt.legend()\n",
    "        plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(\"{}/train/{}.png\".format(foldername, train_state))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        #Plot error\n",
    "        plt.clf()\n",
    "        state_dataset[[modelError, \"NaiveError\"]].plot(figsize=(10,10))\n",
    "        plt.title(\"{}\\n{}\".format(modelError, train_state))\n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.legend()\n",
    "        plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(\"{}/train/{}-Error.png\".format(foldername, train_state))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    test_state = experiment[\"test_state\"]\n",
    "    comparisonCsv[\"{}-Naive-Ratio\".format(modelName)] = comparisonCsv[modelRMSE] / comparisonCsv[\"NaiveRMSE\"]\n",
    "    comparisonCsv[\"Average-Ratio\"] = comparisonCsv[modelRMSE].sum() / comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv[\"Total {}\".format(modelRMSE)] = comparisonCsv[modelRMSE].sum()\n",
    "    comparisonCsv[\"TotalNaive-RMSE\"] = comparisonCsv[\"NaiveRMSE\"].sum()\n",
    "    comparisonCsv.to_csv(\"{}/train/Z_AVERAGE_RMSE.csv\".format(foldername))\n",
    "\n",
    "    #Test\n",
    "    state_population = population[test_state]\n",
    "    state_country = countries[test_state]\n",
    "    data_folder = \"../../data/{}/processed_data/{}\".format(state_country, test_state)\n",
    "    state_dataset = pd.read_csv(data_folder, index_col=0)\n",
    "    x, y = getXY(state_dataset, state_population)\n",
    "\n",
    "    predictions = model.predict(x)\n",
    "    #Transform to 1-D\n",
    "    y = y.reshape((len(y), 1))\n",
    "\n",
    "    #Rescale\n",
    "    inv_yPred = np.apply_along_axis(lambda x: x * state_population / 100000, 1, predictions)\n",
    "    state_dataset[\"Cases\"] *= (state_population / 100000)\n",
    "    naive = state_dataset[\"Cases\"].values[3:-4]\n",
    "    state_dataset = state_dataset[7:\n",
    "\n",
    "    state_dataset[modelPrediction] = inv_yPred\n",
    "    state_dataset[modelError] = state_dataset[modelPrediction] - state_dataset[\"Cases\"]\n",
    "\n",
    "    #Naive\n",
    "    state_dataset[\"Naive-Prediction\"] = naive\n",
    "    state_dataset[\"NaiveError\"] = state_dataset[\"Naive-Prediction\"] - state_dataset[\"Cases\"]\n",
    "\n",
    "    naiveErrorSquared = state_dataset[\"NaiveError\"] ** 2\n",
    "    naiveMSE = naiveErrorSquared.mean()\n",
    "    naiveRMSE = naiveMSE ** (0.5)\n",
    "    NNErrorSquared = state_dataset[modelError] ** 2\n",
    "    NNMSE = NNErrorSquared.mean()\n",
    "    NNRMSE = NNMSE ** (0.5)\n",
    "\n",
    "    testCsv = testCsv.append({\n",
    "            \"File\": test_state,\n",
    "            \"NaiveRMSE\": naiveRMSE,\n",
    "            modelRMSE : NNRMSE,\n",
    "            }, ignore_index=True)\n",
    "    state_dataset.rename(index=str, columns={\"Cases\": \"Observed\"}, inplace=True)\n",
    "    state_dataset.to_csv(\"{}/test/{}\".format(foldername, test_state))\n",
    "\n",
    "    state_dataset[[\"Observed\", modelPrediction, \"Naive-Prediction\"]].plot(figsize=(10,10))\n",
    "    plt.title(\"{} Model\\n{}\".format(modelName, test_state))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"ZIKV Cases\")\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(\"{}/test/{}.png\".format(foldername, test_state))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    #Plot error\n",
    "    plt.clf()\n",
    "    state_dataset[[modelError, \"NaiveError\"]].plot(figsize=(10,10))\n",
    "    plt.title(\"{}\\n{}\".format(modelError,test_state))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.9, 0.9, 0.9, 0.7))\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='dashed', linewidth=1.5)\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(\"{}/test/{}-Error.png\".format(foldername, test_state))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "testCsv[\"{}-Naive-Ratio\".format(modelName)] = testCsv[modelRMSE] / testCsv[\"NaiveRMSE\"]\n",
    "testCsv[\"Average-Ratio\"] = testCsv[modelRMSE].sum() / testCsv[\"NaiveRMSE\"].sum()\n",
    "testCsv[\"Total {}\".format(modelRMSE)] = testCsv[modelRMSE].sum()\n",
    "testCsv[\"TotalNaive-RMSE\"] = testCsv[\"NaiveRMSE\"].sum()\n",
    "testCsv.to_csv(\"./Z_AVERAGE_RMSE.csv\".format(foldername))\n",
    "saveModel(model, \"MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
